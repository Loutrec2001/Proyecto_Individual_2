{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries for scrapping purpose.\n",
    "\n",
    "response = requests.get(\"https://www.coursera.org/courses\")\n",
    "html_soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# we have set the url to scrap and using get method we send a request and then using html.parser we parsed the response content with help of scrapping library called beautiful soup.\n",
    "\n",
    "\n",
    "url = html_soup.find_all(href=True)\n",
    "\n",
    "#find all the URLs (items in the html where href exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def auto_Scrapper(html_tag,course_case):\n",
    "  for i in range(1,100):\n",
    "    url = \"https://www.coursera.org/courses?page=\" +str(i) + \"&index=prod_all_products_term_optimization\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    for j in range(0,9):\n",
    "      x = soup.find_all(html_tag)[j].get_text()\n",
    "      course_case.append(x)\n",
    "        \n",
    "# the function auto_Scrapper is used to get two parameters that is the tag and what to scrap and get the content scrapped.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_Scrapper_Class(html_tag,course_case,tag_class):\n",
    "  for i in range(1,100):\n",
    "    url = \"https://www.coursera.org/courses?page=\" +str(i) + \"&index=prod_all_products_term_optimization\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    for j in range(0,9):\n",
    "        x = soup.find_all(html_tag, class_ = tag_class)[j].get_text()\n",
    "        course_case.append(x)\n",
    "        \n",
    "# the function auto_Scrapper_Class is used to get three parameters that is the tag,what to scrap and get the content scrapped and class it belongs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "course_title = []\n",
    "course_organization = []\n",
    "course_Certificate_type = []\n",
    "course_rating = []\n",
    "course_difficulty = []\n",
    "course_students_enrolled = []\n",
    "\n",
    "# making an empty list so that we can append each of them at the end into a list for making dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m auto_Scrapper(\u001b[39m'\u001b[39;49m\u001b[39mh2\u001b[39;49m\u001b[39m'\u001b[39;49m,course_title)\n\u001b[1;32m      2\u001b[0m auto_Scrapper_Class(\u001b[39m'\u001b[39m\u001b[39mspan\u001b[39m\u001b[39m'\u001b[39m,course_organization,\u001b[39m'\u001b[39m\u001b[39mpartner-name m-b-1s\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m auto_Scrapper_Class(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m,course_Certificate_type,\u001b[39m'\u001b[39m\u001b[39m_jen3vs _1d8rgfy3\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m, in \u001b[0;36mauto_Scrapper\u001b[0;34m(html_tag, course_case)\u001b[0m\n\u001b[1;32m      5\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(page\u001b[39m.\u001b[39mcontent, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m10\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m   x \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39;49mfind_all(html_tag)[j]\u001b[39m.\u001b[39mget_text()\n\u001b[1;32m      8\u001b[0m   course_case\u001b[39m.\u001b[39mappend(x)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "auto_Scrapper('h2',course_title)\n",
    "auto_Scrapper_Class('span',course_organization,'partner-name m-b-1s')\n",
    "auto_Scrapper_Class('div',course_Certificate_type,'_jen3vs _1d8rgfy3')\n",
    "auto_Scrapper_Class('span',course_rating,'ratings-text')\n",
    "auto_Scrapper_Class('span',course_difficulty,'difficulty')\n",
    "auto_Scrapper_Class('span',course_students_enrolled,'enrollment-number')\n",
    "# here we are creating the lists of all data required and appending them to list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_df = pd.DataFrame({'course_title': course_title,\n",
    "                          'course_organization': course_organization,\n",
    "                          'course_Certificate_type': course_Certificate_type,\n",
    "                          'course_rating':course_rating,\n",
    "                           'course_difficulty':course_difficulty,\n",
    "                           'course_students_enrolled':course_students_enrolled})\n",
    "courses_df = courses_df.sort_values('course_title')\n",
    "\n",
    "# here we take each lists we generated by scrapping and made a dataframe out of it isung pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "courses_df.to_csv('Coursera_Cours.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
